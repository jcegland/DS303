---
title: "HW09"
author: "Jillian Egland"
date: "2023-10-31"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
```{r, message=FALSE}
library(ISLR2)
library(class)
library(caret)
library(MASS)
library(ROCR)
library(e1071)
```

# Problem 1: MNIST handwritten digit database
## 1a
*Randomly select 3000 observations from the training set and randomly select 100 observations from the test set. Implement KNN classification. Report the following:*  
*Carry out 10-fold cross-validation on the training set to determine the optimal K. Try K = 1,5,7,9. What is the optimal K?*  
*Use this optimal K to implement KNN classification on the test set. Report your confusion matrix and misclassification error rate on the test set.*  

```{r sourceCode}
# use source() function to source the functions we want to execute
source("mnist_load_script.R")
```

```{r, results='hide'}
# modified to load fashion mnist as fashion_train and fashion_test 
load_mnist() 

```
optimal k is 5  
misclassification rate: 0.08   
confusion matrix:  
```{r}
# choosing 3000 observations for training and 100 for test 
index.train = sample(1:dim(train$x)[1], 3000, replace=FALSE)
train$x=train$x[index.train,]
train$y=train$y[index.train]
index.test = sample(1:dim(test$x)[1], 100, replace=FALSE)
test$x=test$x[index.test,]
test$y=test$y[index.test]

flds <- createFolds(train$y, k = 10, list = TRUE, returnTrain = FALSE)
#names(flds)

K= c(1,5,7,9) # nearest neighbor values

cv_error = matrix(NA, 10, 4)

for(j in 1:4){
  k = K[j]
  for(i in 1:10){
    test_index = flds[[i]]
    testX = train$x[test_index,]
    trainX = train$x[-test_index,]
    
    trainY = train$y[-test_index]
    testY = train$y[test_index]
    
    knn.pred = knn(trainX,testX,trainY,k=k)
    cv_error[i,j] = mean(testY!=knn.pred)
  }
}

#mean of the columns 
#apply(cv_error,2,mean)
bestKIndex = which.min(apply(cv_error,2,mean))

knn.pred = knn(train$x,test$x,train$y,k=K[bestKIndex])
#mean(test$y!=knn.pred)

table(knn.pred,test$y)

```

## 1b 
*Try to implement LDA on the MNIST dataset. What kind of error message do you obtain? Do some digging and explain what this error message means.*  

LDA 
```{r}
lda.fit = lda(train$y~train$x)


```
Some of the limitations of LDA are if there is substantial separation between the classes, if it is hight dimensional data (p>n), with irregular decision boundaries, or if there are more than 2 categories. I would guess that this error is because there are more than 2 categories.  


## 1c 
*Discuss how this dataset highlights some of the advantages of using KNN for classification.*  

KNN has advantages for classification because there are no assumptions as it is completely data driven. It also works well with irregular decision boundaries and when there are more than 2 classification types. So KNN works well for this dataset where Y can be 0,1,...,9  




# Problem 2: Fashion MNIST
## 2a 
*Produce plots of the first 5 observations in the training set. What do you see?*  

The plots produce pictures of clothing items: ankle boot, t-shirt, t-shirt, dress, t-shirt.  
```{r}
#head(fashion_train$y)

#dim(fashion_train$x)
par(mfrow = c(2, 3))
# each observations represents a 28 x 28 pixel image (we treat it as 784 dimensional observation)
show_digit(fashion_train$x[1,])
show_digit(fashion_train$x[2,])
show_digit(fashion_train$x[3,])
show_digit(fashion_train$x[4,])
show_digit(fashion_train$x[5,])
##fashion_train$y[1]
##fashion_train$y[2]
##fashion_train$y[3]
##fashion_train$y[4]
##fashion_train$y[5]

```
## 2b 
*Repeat Problem 1(a) for this dataset. How do your confusion matrices and misclassification error rates compare?*  

optimal k is 7  
misclassification rate: 0.26    
confusion matrix:  
```{r}
# choosing 3000 observations for training and 100 for test 
index.train = sample(1:dim(fashion_train$x)[1], 3000, replace=FALSE)
fashion_train$x=fashion_train$x[index.train,]
fashion_train$y=fashion_train$y[index.train]
index.test = sample(1:dim(fashion_test$x)[1], 100, replace=FALSE)
fashion_test$x=fashion_test$x[index.test,]
fashion_test$y=fashion_test$y[index.test]

flds <- createFolds(fashion_train$y, k = 10, list = TRUE, returnTrain = FALSE)
#names(flds)

K= c(1,5,7,9) # nearest neighbor values

cv_error = matrix(NA, 10, 4)

for(j in 1:4){
  k = K[j]
  for(i in 1:10){
    test_index = flds[[i]]
    testX = fashion_train$x[test_index,]
    trainX = fashion_train$x[-test_index,]
    
    trainY = fashion_train$y[-test_index]
    testY = fashion_train$y[test_index]
    
    knn.pred = knn(trainX,testX,trainY,k=k)
    cv_error[i,j] = mean(testY!=knn.pred)
  }
}

#mean of the columns 
#apply(cv_error,2,mean)
bestKIndex = which.min(apply(cv_error,2,mean))

knn.pred = knn(fashion_train$x,fashion_test$x,fashion_train$y,k=K[bestKIndex])
#mean(fashion_test$y!=knn.pred)

table(knn.pred,fashion_test$y)

```
The confusion matrix looks similar. The misclassification rate is worse, but still low.  


# Problem 3: Concept Review 
## 3a 
*Suppose you just took on a new consulting client. He tells you he has a large dataset (say 100, 000 observations) and he wants to use this to classify whether or not to invest in a stock based on a set of p = 10, 000 predictors. He claims KNN will work really well in this case because it is non-parametric and therefore makes no assumptions on the data. Present an argument to your client on why KNN might fail when p is large relative to the sample size.*  

When the data is high dimensional (p>n), the data points are further apart which makes distance not as meaningful. The whole point of KNN is to find the closest points and assume that this point is similar to them, but if the points are all far away, this assumption breaks down.  

## 3b 
*For each of the following classification problems, state whether you would advise a client to use LDA, logistic regression, or KNN and explain why:*  
*We want to predict gender based on height and weight. The training set consists of heights and weights for 82 men and 63 women.*  
i) LDA, you can assume that height and weight will have a normal distribution and p is slightly larger relative to n 

*We want to predict gender based on annual income and weekly working hours. The training set consists of 770 mean and 820 women.*  
ii) logistic regression would work better because p is smaller relative to n  

*We want to predict gender based on a set of predictors where the decision boundary is complicated and highly non-linear. The training set consists of 960 men and 1040 women.*  
iii) KNN works really well if the decision boundary is irregular or non-linear.  


# Problem 4: k-NN
*Assume our outcome Y can take on Y =0, Y=1, or Y=2 (3 categories). Suppose we have a training data set with 5 observations. We want to classify a test observation using KNN. Below are all the distances between each of the 5 observations in training set and the test observation.*  

## 4a 
*Based on the above, how would we classify our test observation using K = 1?*. 

The 1 closest observation is 2 which is a 1, so y would be classified as a 1.  

## 4b 
*How would we classify our test observation using K = 3?*  

The 3 closest observations are 2, 4, and 1 which are 1, 0, and 0 respectively so y would be classified as a 0.  

## 4c 
*KNN is highly dependent on the choice of K. Discuss the bias/variance tradeoff we make in choosing K.*  
As k increases, the model becomes less flexible-meaning the bias increases and the variance decreases. With k=1, the model will be super flexible with low bias and high variance because the model would be very close to the truth but would change greatly with differing training sets. Since it only looks at the first nearest neighbor, it is highly vulnerable to change or variance. The opposite is true for a large k. Looking at a large number of neighbors will make it robust to variance. 


# Problem 5: Email Spam Part 2
*Use the Spam data set, from HW 8, for this problem. Repeat your code from Problem 2 parts (a), (b), and (c).*  

## 5a 
*What type of mistake do we think is more critical here: reporting a meaningful email as spam (false positive) or a spam email as meaningful (false negative)?*  

It would be worse to label an email as spam when it wasn't than to label an email as not spam when it was spam, so false positive. 
```{r, results = 'hide', warning=FALSE}
spam = read.csv("/Users/jillianeglandschool/Desktop/DS303/spambase.data",header=FALSE)
set.seed(10)
n = dim(spam)[1]
train_index = sample(1:n,n/2,replace=F)
train_spam = spam[train_index,]
test_spam = spam[-train_index,] 


# roughly the same proportions 
table(train_spam$V58)/dim(train_spam)[1]




glm.fit = glm(V58~., data=train_spam, family='binomial')
glm.prob = predict(glm.fit,test_spam,type='response') 


```

## 5b 
*Fit a logistic regression model here and apply it to the test set. Based on your answer to part (a), plot the ROC curve of true positive rate vs. false positive rate or true negative rate vs. false negative rate.*  

ROC curve:  
```{r}
ROCRpred <- prediction(glm.prob,test_spam$V58)
plot(performance(ROCRpred,'tpr','fpr'),colorize=TRUE,
     print.cutoffs.at=seq(0,1,by=0.1), tzext.adj=c(-0.2,1.7))

perf = performance(ROCRpred,'tpr','fpr')

thresholds = data.frame(threshold = perf@alpha.values[[1]],fpr = perf@x.values[[1]], tpr = perf@y.values[[1]])

fpr0.03 = subset(thresholds,fpr<0.03)

## obtain area under the curve, evaluates overall performance of a classifier
## this can be useful if you are comparing multiple classifiers

#auc_ROCR <- performance(ROCRpred, measure = "auc")
#auc_ROCR@y.values[[1]] 

```


## 5c 
*Output the confusion matrix. What is the false positive and false negative rate when we set the threshold to be 0.5?*  

false positive rate at .5: .0455  
false negative rate at .5: .1141  
confusion matrix: 
```{r}
glm.pred = rep('0',nrow(test_spam))
glm.pred[glm.prob >0.5] ='1'
table(glm.pred,test_spam$V58)

#1-mean(glm.pred == test_spam$V58)

#rows are predicted, # columns are true 

# false positive rate: FP/(FP+TN) = 64/(64+1343) = .0455  
# false negative rate: FN/(FN+TP) = 102/(102+792) = .1141  


```

## 5d 
*Adjust the threshold such that your chosen error (false positive or false negative) is no more than 0.03. You should choose the threshold carefully so that the true positive and true negative rate are also maximized.Report that threshold here.*  

choose a threshold of 0.6722220 This will result in a false positive rate of 0.02985075
```{r, results='hide'}
tail(fpr0.03)

```

## 5e 
*Implement LDA and repeat parts (b) -(d).*  

LDA 
```{r}
lda.fit = lda(V58~.,data=train_spam)
lda.pred = predict(lda.fit,newdata=test_spam, type='class')$posterior[,"1"]

######error here####### 
ROCRpred <- prediction(lda.pred,test_spam$V58)
plot(performance(ROCRpred,'tpr','fpr'),colorize=TRUE,
     print.cutoffs.at=seq(0,1,by=0.1), tzext.adj=c(-0.2,1.7))

perf = performance(ROCRpred,'tpr','fpr')

thresholds = data.frame(threshold = perf@alpha.values[[1]],fpr = perf@x.values[[1]], tpr = perf@y.values[[1]])

fpr0.03 = subset(thresholds,fpr<0.03)

```

false positive rate at .5: .0426  
false negative rate at .5: .2260  
confusion matrix: 
```{r}
lda.pred = predict(lda.fit,newdata=test_spam, type='class')
table(lda.pred$class,test_spam$V58)

# false positive rate: FP/(FP+TN) = 60/(60+1347) = .0426  
# false negative rate: FN/(FN+TP) = 202/(202+692) = .2260   

```

choose a threshold of 0.6615166 This will result in a false positive rate of 0.02914001
```{r, results='hide'}
tail(fpr0.03)

```



## 5f 
*Carry out QDA, Naive Bayes and KNN on the training set. You should experiment with values for K in the KNN classifier using cross-validation. Remember to standardize your predictors for KNN. For each classifier, report the confusion matrix and overall test error rates for each of the classifiers.*  

QDA:  
false positive rate at .5: .2331  
false negative rate at .5: .0481  
confusion matrix:  
```{r}
qda.fit = qda(V58~.,data=train_spam)
qda.pred = predict(qda.fit,test_spam)

table(qda.pred$class,test_spam$V58)
#mean(qda.pred$class!=test_spam$V58)

# false positive rate: FP/(FP+TN) = 328/(328+1079) = .2331 
# false negative rate: FN/(FN+TP) = 43/(43+851) = .0481 

```


Naive Bayes:  
false positive rate at .5: .4229  
false negative rate at .5: .0492  
confusion matrix: 
```{r}
nb.fit = naiveBayes(V58~., data=train_spam)
nb.class = predict(nb.fit, test_spam)
table(nb.class, test_spam$V58)
#mean(nb.class == test_spam$V58) 

# false positive rate: FP/(FP+TN) = 595/(595+812) = .4229 
# false negative rate: FN/(FN+TP) = 44/(44+850) = .0492   

```


KNN:  
false positive rate at .5: .1720  
false negative rate at .5: .2752  
confusion matrix: 
```{r}
standardized.X2 = scale(train_spam[,-58]) #train x
train.X = train_spam[,-58] 
train.Y = train_spam$V58 
test.X = test_spam[,-58]
test.Y = test_spam$V58 


flds <- createFolds(train_spam$V58, k = 10, list = TRUE, returnTrain = FALSE)

K= c(1,3,5,7) # nearest neighbor values

cv_error = matrix(NA, 10, 4)

for(j in 1:4){
  k = K[j]
  for(i in 1:10){
    test_index = flds[[i]]
    testX = standardized.X2[test_index,]
    trainX = standardized.X2[-test_index,]
    
    trainY = train_spam$V58[-test_index]
    testY = train_spam$V58[test_index]
    
    knn.pred = knn(trainX,testX,trainY,k=k)
    cv_error[i,j] = mean(testY!=knn.pred)
  }
}


bestKIndex = which.min(apply(cv_error,2,mean))

knn.pred = knn(train.X,test.X,train.Y,k=K[bestKIndex])
#mean(test.Y!=knn.pred)

table(knn.pred,test.Y)

# false positive rate: FP/(FP+TN) = 242/(242+1165) = .1720  
# false negative rate: FN/(FN+TP) = 246/(246+648) = .2752    


```

## 5g 
*Which classifier would you recommend for this data? Justify your answer.*  

Since we are focused on minimizing false positive rates, I would choose LDA since it has the lowest false positive rate at .0426  



















