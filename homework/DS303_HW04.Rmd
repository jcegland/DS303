---
title: "HW04"
author: "Jillian"
date: "2023-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Problem 1: Best subset selection
```{r, results = 'hide', message = FALSE }
prostate = read.table("/Users/jillianeglandschool/Desktop/DS303/prostate.data",header=TRUE) 
library(kableExtra)
library(leaps)
library(caret)
```
*The data for this problem comes from a study by Stamey et al. (1989). They examined the relationship between the level of prostate-specific antigen and a number of clinical measures in men who were about to receive a radical prostatectomy. The variables are log cancer volume (lcavol), log prostate weight (lweight), age, log of the amount of benign prostatic hyperplasia (lbph), seminal vesicle invasion (svi), log of capsular penetration (lcp), Gleason score (gleason), and percent of Gleason scores 4 or 5 (ppp45). The last column corresponds to which observations were used in the training set and which were used in the test set (train).*  
*Our response of interest here is the log prostate-specific antigen (lpsa). We will use this data set to practice 3 common subset selection approaches.*  

## 1a 
*Approach 1: Perform best subset selection on the entire data set with lpsa as the response. For each model size, you will obtain a ’best’ model (size here is just the number of predictors in the model): M1 is the best model with 1 predictor (size 1), M2 is the best model with 2 predictors (size 2), and so on. Create a table of the AIC, BIC, adjusted R2 and Mallow’s Cp for each model size. Report the model with the smallest AIC, smallest BIC, largest adjusted R2 and smallest Mallow’s Cp. Do they lead to different results? Using your own judgement, choose a final model.*  

Best model by BIC: M3  
Best model by AIC: M5  
Best model by Mallow's Cp: M5  
Best model by adj R^2: M7  
final best model: M5   
I would choose Model 5 as 2 different methods recommended it. It is also slightly simpler which would be more cost effective than gathering more medical data.  

```{r, results = 'hide'}
regfit = regsubsets(lpsa~.-train,data=prostate,nbest=1,nvmax=8)
regfit.sum = summary(regfit)
regfit.sum
#names(regfit.sum)

n = dim(prostate)[1]
p = rowSums(regfit.sum$which)
adjr2 = regfit.sum$adjr2
cp = regfit.sum$cp
rss = regfit.sum$rss
# use our own formulas for AIC and BIC 
AIC = n*log(rss/n) + 2*(p)
BIC = n*log(rss/n) + (p)*log(n)

cbind(p,rss,adjr2,cp,AIC,BIC)
#plot(p,BIC)
#plot(p,AIC)

which.min(BIC)
which.min(AIC)
which.min(cp)
which.max(adjr2)

```
```{r}
kable(coef(regfit,3), format = "html", caption='M3') %>% kable_styling()
kable(coef(regfit,5), format = "html", caption='M5') %>% kable_styling()
kable(coef(regfit,6), format = "html", caption='M6') %>% kable_styling()


```


## 1b 
*Approach 2: The dataset has already been split into a training and test set. Construct your training and test set based on this split. You may use the following code for convenience:*  
*For each model size, you will obtain a ‘best’ model. Fit each of those models on the training set. Then evaluate the model performance on the test set by computing their test MSE. Choose a final model based on prediction accuracy. Fit that model to the full dataset and report your final model here.*  

This method picks model 3 as the best model for having the best test MSE.  
Model 3: Y = -1.0227780 + 0.5199861(lcavol) + 0.7367954(lweight) + 0.5379032(svi)  

```{r, results = 'hide'}
train_data = subset(prostate,train==TRUE)[,1:9]
test_data = subset(prostate,train==FALSE)[,1:9]


best.train = regsubsets(lpsa~.,data=prostate,nbest=1,nvmax=8)

## the predict() function does not work on regsubsets objects, 
# so we need to write our own code to obtain our predicted values:

val.errors = rep(NA,8)
for(i in 1:8){
  test.mat = model.matrix(lpsa~.,data=test_data)
  
  coef.m = coef(best.train,id=i)
  
  pred = test.mat[,names(coef.m)]%*%coef.m
  val.errors[i] = mean((test_data$lpsa-pred)^2)
}

which.min(val.errors)

coef(best.train,id=3)

```
```{r}
kable(coef(regfit,3), format = "html", caption='M3') %>% kable_styling()


```


## 1c 
*Approach 3: This approach is used to select the optimal size, not which predictors will end up in our model. Split the dataset into k folds (you decide what k should be). We will perform best subset selection within each of the k training sets. Here are more detailed instructions:*  
*i. For each fold k = 1,...,K:*  
  *1. Perform best subset selection using all the data except for those in fold k (training set). For each model size, you will obtain a ‘best’ model.*  
  *2. For each ‘best’ model, evaluate the test MSE on the data in fold k (test set). *  
  *3. Store the test MSE for each model.*  
  
*Once you have completed this for all k folds, take the average of your test MSEs for each model size. In other words, for all k models of size 1, you will compute their k- fold cross-validated error. For all the k models of size 2, you will compute their k-fold cross-validated errors, and so on. Report your 8 CV errors here.*  
*ii. Choose the model size that gives you the smallest CV error. Now perform best subset selection on the full data set again in order to obtain this final model. Report that model here. (For example, suppose cross-validation selected a 5-predictor model. I would perform best subset selection on the full data set again in order to obtain the 5-predictor model.)*  

k folds subset approach chose a size 3 model as the best model.  

```{r, results='hide'}
k = 10 
flds <- createFolds(prostate$lpsa, k = 10, list = TRUE, returnTrain = FALSE)

val.errors = matrix(NA, k, 8)
# loop over k folds (for j in 1:k)
for (j in 1:k){
  test_index = flds[[i]]
  
  test = prostate[test_index,-10]
  train = prostate[-test_index,-10]
  
  best.fit = regsubsets(lpsa~.,data=train,nbest=1,nvmax=8) 
  
  for (i in 1:8){
    test.mat = model.matrix(lpsa~.,data=test)
    
    coef.m = coef(best.fit,id=i)
    
    pred = test.mat[,names(coef.m)]%*%coef.m
    val.errors[j,i] = mean((test$lpsa-pred)^2)
    
  }
}

cv.errors = apply(val.errors,2,mean)
which.min(cv.errors)

best_mod = regsubsets(lpsa~.,data=prostate,nbest=1,nvmax=8) 

```

```{r} 
kable(cv.errors, format = "html", caption='cv errors') %>% kable_styling()
kable(coef(best_mod,id=3), format = "html", caption='M3') %>% kable_styling()


```


# Problem 2: Simulation Studies 
*We have seen that as the number of features used in a model increases, the training error will necessarily decrease, but the test error may not. We will now explore this in a simulated data set.*  

## 2a 
*Generate a data set with p = 20 features, n = 1000 observations, and an associated response vector Y generated according to the model: Y =Xβ+ε,*  
*where β is a vector of population parameters that has some elements that are exactly equal
to zero. Note you decide what β is and which are equal to 0.*  

generating dataset: all betas except for 1,2,3,4,5,6,10,11 are equal to 0.  
```{r}
#set.seed(1)
p=20 
n=1000 
X = matrix(NA, nrow=1000, ncol=20)
  for (i in 1:20){
    X[,i] = rnorm(1000)
  }
X = cbind(rep(1,1000), X)

betas = c(0,5,3,6,9,4,0,0,0,2,1,0,0,0,0,0,0,0,0,0,0)
error = rnorm(1000,0,1)
Y = X%*%betas + error 


```

# 2b 
*Split your data set into a training set containing 100 observations and a test set containing
900 observations.*  

split into test (size 100) and train (size 900). 
```{r}
test_index = sample(1:1000, 100, rep=FALSE)
df = data.frame(X, Y)
test = data.frame(Y = Y[test_index], X[test_index,])
train = data.frame(Y = Y[-test_index], X[-test_index,])
#test = df[test_index, ]
#train = df[-test_index, ]

```

# 2c 
*Perform best subset selection on the training set, and plot the training set MSE associated with the best model of each size.*  

Training MSE for each best model:  
best training MSE is model 20.  
```{r}
regfit = regsubsets(Y~.-X1, data=train, nbest=1,nvmax=20)
regfit.sum = summary(regfit) 
plot(regfit.sum$rss/20)
#which.min(regfit.sum$rss/20)

```

# 3d 
*Plot the test set MSE associated with the best model of each size.*  

Test MSE for each best model:  
```{r}
## finds test MSE 
val.errors = rep(NA,20)
for(i in 1:20){
  test.mat = model.matrix(Y~.,data=test)
  
  coef.m = coef(regfit,id=i)
  
  pred = test.mat[,names(coef.m)]%*%coef.m
  val.errors[i] = mean((test$Y-pred)^2)
}

plot(val.errors)

```

# 3e 
*For which model size does the test set MSE take on its minimum values? Comment on your results. If it takes on its minimum value for a model containing only an intercept or a model containing all of the features, then play around with the way that you are generating the data in (a) until you come up with a scenario in which the test set MSE is minimized for an intermediate model size.*  

Model 8 has the minimum test MSE value. It did not continue to decrease when more predictors were added like the training MSE did. 
```{r, results = 'hide'}
which.min(val.errors)
#coef(regfit,id=which.min(val.errors)) 

```

# 3f 
*How does the model at which the test set MSE is minimized compare to the true model used to generate the data? Comment on the regression coefficient values.*  

The model with the best test MSE is a model with 8 predictors, which is the same as the true model. The true model had 8 predictors with the rest of the betas equal to 0. The predictors picked by the best model were the same predictors in the true model that weren't equal to 0, with very close to the true beta values. 
```{r}
coef(regfit,id=which.min(val.errors)) 
#summary(regfit)

```



# Problem 3: Cross-validation 

# 3a 
*Explain how k-fold cross-validation is implemented.*  

K folds separate the dataset into k sections. It will loop k times, and each time, one of the k sections is used as the test set, with the other k-1 sections as the training set. A model is then fit using the training set, and a Test MSE is calculated using a prediction from the training set and the values in the test set.  

# 3b 
*What are the advantages and disadvantages of k-fold cross-validation relative to: The validation set approach?, LOOCV? * 
Validation set approach: advantage of k-fold is we get to utilize more data for training.testing purposes. The disadvantage is that it is not as easy to implement and can be more computationally intensive to implement.  
LOOCV: advantage is that k-fold is less computationally intensive than LOOCV. Disadvantage is the k-fold untilizes less data and it still has randomness.  

# 3c 
*For the following questions, we will perform cross-validation on a simulated data set. Generate a simulated data set such that Y = X−2X^2+ε, with ε∼N(0,12). Fill in the following code:* 

```{r}
set.seed(1)
x = rnorm(100)
error = rnorm(100,0,1^2)
y = x - 2*x^2 + error 

```

# 3d 
*Fit a linear model to the data set you simulated (y ∼ x) and check whether or not the linearity assumption holds. Present the corresponding diagnostic plot and interpret what you observe.*  

The linearity assumption does not hold. The residual plot clearly shows a curve on the red line, indicating that the model is not linear. 
```{r}
fit = lm(y~x)
sum.fit = summary(fit)

par(mfrow=c(2,2))
plot(fit)

``` 

# 3e 
*Set a random seed, and then compute the LOOCV errors that result from fitting the following 4 models using lm and poly:*  

LOOCV errors: 
```{r}
df = data.frame(x, y)
set.seed(12) 

n = dim(df)[1]
MSE_M1 = MSE_M2 = MSE_M3 = MSE_M4 = rep(0,n)
for(i in 1:n){
  test = df[i,]
  train = df[-i,]
  
  M1 = lm(y~x,data=train)
  M2 = lm(y~poly(x,2),data=train)
  M3 = lm(y~poly(x,3),data=train)
  M4 = lm(y~poly(x,4),data=train)
  
  M1_y = predict(M1,newdata=test)
  M2_y = predict(M2,newdata=test)
  M3_y = predict(M3,newdata=test)
  M4_y = predict(M4,newdata=test)
  
  MSE_M1[i] = (test$y - M1_y)^2
  MSE_M2[i] = (test$y - M2_y)^2
  MSE_M3[i] = (test$y - M3_y)^2
  MSE_M4[i] = (test$y - M4_y)^2
}

mean(MSE_M1)
mean(MSE_M2)
mean(MSE_M3)
mean(MSE_M4)


``` 

# 3f 
*Repeat the above step using another random seed, and report your results. Are your results the same as what you got in (d). Why?*  

The results were exactly the same. This is because LOOCV is consistent. Each observation gets a chance to be the test set one time, so the value, which is the average of all the error of each test set, will come out the same every time.  
```{r, results = 'hide'}
df = data.frame(x, y)
set.seed(21) 

n = dim(df)[1]
MSE_M1 = MSE_M2 = MSE_M3 = MSE_M4 = rep(0,n)
for(i in 1:n){
  test = df[i,]
  train = df[-i,]
  
  M1 = lm(y~x,data=train)
  M2 = lm(y~poly(x,2),data=train)
  M3 = lm(y~poly(x,3),data=train)
  M4 = lm(y~poly(x,4),data=train)
  
  M1_y = predict(M1,newdata=test)
  M2_y = predict(M2,newdata=test)
  M3_y = predict(M3,newdata=test)
  M4_y = predict(M4,newdata=test)
  
  MSE_M1[i] = (test$y - M1_y)^2
  MSE_M2[i] = (test$y - M2_y)^2
  MSE_M3[i] = (test$y - M3_y)^2
  MSE_M4[i] = (test$y - M4_y)^2
}

mean(MSE_M1)
mean(MSE_M2)
mean(MSE_M3)
mean(MSE_M4)


```

# 3g 
*Which of the models in (d) had the smallest LOOCV error? Is this what you expected? Explain your answer.*  

The smallest LOOCV error was from the quadratic model. This is what I expected; since the true model was quadratic, the best model should also be quadratic.  


# 3h 
*Comment on the statistical significance of the coefficient estimates that results from fitting each of the models in (d) using least squares. Do these results agree with the conclusions drawn based on the cross-validation results?*  

For the models that had coefficients past the second exponential, those coefficients were not statistically significant. This agrees with the results from the cross validation that indicated that the quadratic model was the best model. Any coefficients past x^2 would not be statistically significant.  

```{r, results = 'hide'}
M1 = lm(y~x,data=df)
M2 = lm(y~poly(x,2),data=df)
M3 = lm(y~poly(x,3),data=df)
M4 = lm(y~poly(x,4),data=df)

summary(M1) 
summary(M2) 
summary(M3) 
summary(M4) 


```



# Problem 4: Concept Review 

# 4a 
*For the following statement, state whether or not it is True or False. Briefly justify your answer. Suppose I have 3 models to pick from: (not nested predictors, same number of predictors) *  
*Using AIC, BIC, Mallow’s Cp, adjusted R2 could lead us to pick different final models.*  

False, they will lead us to pick the same model. Since they each have the same number of predictors, the penalty will be constant for each.  

# 4b 
*For the following statement, state whether or not it is True or False. Briefly justify your answer. Suppose I have 2 models to pick from: (not nested predictors) *  
*It must be that RSS3 ≥ RSS4*  

False, RSS4 only decreases from RSS3 when they are nested, meaning that RSS4 has the same predictors as RSS3 but with an additional predictor. Since M4 doesn't include X1 and X2, they are not nested, and the statement does not hold.  

# 4c 
*For the following statement, state whether or not it is True or False. Briefly justify your answer. Suppose I have 2 models to pick from: (nested predictors) *  
*It must be that RSS2 ≥ RSS3*  

True, since M2 is nested in M4, this is true. We have a RSS for M2, adding a predictor will either decrease the RSS or stay the same. This is true for adding a second extra predictor. So the statement holds.  

# 4(a) 
*Subset selection will produce a collection of p models M1,M2,...,Mp. These represent the ‘best’ model of each size (where ‘best’ here is defined as the model with the smallest RSS). Is it true that the model identified as Mk+1 must contain a subset of the predictors found in Mk? In other words, is it true that if M1 : Y ∼ X1, then M2 must also contain X1. And if M2 contains X1 and X2, then M3 must also contain X1 and X2? Explain your answer.*  

No, that is false. Mk will not necessarily be nested in Mk+1. In this exhaustive search, every combination is searched so it doesn't just find the next best variable to add. It checks the RSS of every combination of that size. So if the interaction between the variables when Mk+1 doesn't include X1 even though Mk does results in a smaller RSS, that will be chosen as the best model for the size.  

# 4(b) 
*What advantages are there to using AIC/BIC instead of using the test MSE as our model selection criteria? Explain.*  

Test MSE means we have to split our data into training and test subsets. However, this means that not all of our data is leveraged in the model. We will have different data every time since it is a random split, so it's inconsistent. AIC/BIC will be more consistent and will leverage all the data. Both of these will also either choose the true model every time or find the one that mimics the true model the closest. AIC/BIC should be used when there is a small sample size, and you cannot afford to split your data into different sets.  

# 4(c) 
*Suppose your colleague fit a multiple linear regression model on a dataset with response Y and p predictors X1, X2, . . . , Xp. Their p-value for one of the predictors is 0.0647. It is not significant at α = 0.05 so your colleague claims that the predictor is not meaningful and suggests fitting a model without this predictor. Do you agree or disagree with their claim? Justify your answer.*  

Disagree, looking at individual p values is not always the most reliable. It may be that the interaction of those specific variables is an accurate representation of the true model. The more accurate assessment of the model would be looking at it's overall values, such as it's overall p value, test MSE, adj R^2, or AIC/BIC values.  



