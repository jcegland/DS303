---
title: "HW07"
author: "Jillian Egland"
date: "2023-10-10"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r, results = 'hide', message=FALSE}
library(ISLR2)
library(leaps)
library(ggplot2)
library(kableExtra)
library(car)
library(glmnet)
```

# Problem 1: Concept Review 
## 1a 
The shape of the l1 penalty used in lasso regression creates a diamond since the penalty is the equation for a diamond, while the l2 penalty used in ridge regression is the equation for a circle. The least squares solution can touch the corner of the diamond but can only get close to the circle. Touching the corner of the diamond shrinks the predictor variable to 0.  

## 1b 
a) as lambda increases, training MSE will:  
   ii. decrease initially, then eventually start increasing in a U shape, a larger lambda will increase the model's prediction, but adding too much bias will result in moving the model towards underfitting which will increase training MSE. 

b) as lambda increases, test MSE will:  
   ii. decrease initially, then eventually start increasing in a U shape, larger lambda means adding more bias into the model which will decrease the test MSE by decreasing variance, but adding too much bias will result in a higher test MSE  

c) as lambda increases, variance will:  
   iv. steadily decrease, larger lambda means adding more bias into the model, which reduces the variance  

d) as lambda increases, (squared) bias will:  
   iii. steadily increase, larger lambda means we are adding more bias into the model so bias will increase  

e) as lambda increases, irreducible error will:  
   v. remain constant, irreducible error is not affected by bias in the model  


# Problem 2: Regularized Regression Models

```{r}
Hitters = na.omit(Hitters)
n = nrow(Hitters) #there are 263 observations
x = model.matrix(Salary ~.,data=Hitters)[,-1]  #19 predictors
Y = Hitters$Salary
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test=(-train)
Y.test = Y[test]

```

## 2b 
minimum lambda for ridge regression:  
```{r}
grid = 10^seq(10,-2,length=100)
ridge.train = glmnet(x[train,],Y[train],alpha=0,lambda=grid)

### select an optimal lambda 
set.seed(1)
cv.out = cv.glmnet(x[train,],Y[train],alpha = 0, lambda = grid, nfolds=10) 
#default performs 10-fold CV, but you can change this using the argument `nfolds` 

lambda_min_ridge = cv.out$lambda.min
lambda_min_ridge

plot(cv.out)
```

## 2c 
lambda value where cv error is within 1 se:  
```{r}
lambda_1se_ridge = cv.out$lambda.1se 
lambda_1se_ridge 

```


## 2d 
```{r}
lasso.train = glmnet(x[train,],Y[train],alpha=0,lambda=grid)

### select an optimal lambda 
set.seed(1)
cv.out = cv.glmnet(x[train,],Y[train],alpha = 1, lambda = grid, nfolds=10) 
#default performs 10-fold CV, but you can change this using the argument `nfolds` 

lambda_min_lasso = cv.out$lambda.min
lambda_min_lasso

plot(cv.out)

```

## 2e 
lambda value where cv error is within 1 se:  
```{r}
lambda_1se_lasso = cv.out$lambda.1se 
lambda_1se_lasso 

```

## 2f  
ridge min cv, ridge 1se, lasso min cv, lasso 1se test MSEs respectively: 
```{r}
ridge.pred = predict(ridge.train,s=lambda_min_ridge,newx=x[test,])
mean((ridge.pred-Y.test)^2)
ridge.1se.pred = predict(ridge.train,s=lambda_1se_ridge,newx=x[test,])
mean((ridge.1se.pred-Y.test)^2)

lasso.pred = predict(lasso.train,s=lambda_min_lasso,newx=x[test,])
mean((lasso.pred-Y.test)^2)
lasso.1se.pred = predict(lasso.train,s=lambda_1se_lasso,newx=x[test,])
mean((lasso.1se.pred-Y.test)^2)

```

## 2g 
The regression coefficients for lambda_min_ridge are generally larger than for lambda_1se_ridge.  
The regression coefficients for lambda_min_lasso are generally larger than for lambda_1se_lasso. Lambda_min_lasso kept more predictors than lambda_1se_lasso did.  
The regression coefficients for lasso that weren't shrunk to 0 were larger than their corresponding ridge coefficients.  
The regression coefficients for the min lambda were larger than the ones for the 1se lambda.  

This means that lasso coefficients are larger to offset the ones that they shrink to 0 so they are larger than for ridge. The 1se lambda shrinks the coefficients more than the min lambda does.  


```{r}
final_ridge_min = glmnet(x,Y,alpha=0,lambda = lambda_min_ridge)
final_ridge_min = data.frame(coef(final_ridge_min)[,1])

final_ridge_1se = glmnet(x,Y,alpha=0,lambda = lambda_1se_ridge)
final_ridge_1se = data.frame(coef(final_ridge_1se)[,1])

final_lasso_min = glmnet(x,Y,alpha=1,lambda = lambda_min_lasso)
final_lasso_min = data.frame(coef(final_lasso_min)[,1])

final_lasso_1se = glmnet(x,Y,alpha=1,lambda = lambda_1se_lasso)
final_lasso_1se = data.frame(coef(final_lasso_1se)[,1])


final_coef = cbind(final_ridge_min, final_ridge_1se, final_lasso_min, final_lasso_1se) 
kable(final_coef, format = "html", caption='') %>% kable_styling()

```

## 2h 
alpha_enet and lambda_enet are: 
```{r}
set.seed(1)
alphas = seq(0.01, 0.99, .01)
cv_error = rep(NA, length(alphas))

for (i in 1:length(alphas)){
  set.seed(1)
  cv_elastic = cv.glmnet(x[train,],Y[train],alpha=alphas[i],lambda=grid)
  cv_error[i] = min(cv_elastic$cvm)
  
}
alpha_enet = alphas[which.min(cv_error)]

# finding optimal lambda with best_alpha 
set.seed(1)
elastic_cv = cv.glmnet(x[train,], Y[train], alpha = alpha_enet, lambda=grid)
lambda_enet = elastic_cv$lambda.min 

alpha_enet
lambda_enet

```


## 2i 
test MSE for model with alpha_enet and lambda_enet:  
```{r}
enet.train = glmnet(x[train,], Y[train], alpha = alpha_enet, lambda=grid)

enet.pred = predict(enet.train,s=lambda_enet,newx=x[test,])
mean((enet.pred-Y.test)^2)

# other test MSEs 
#140081.9
#165523.3
#142574.3
#142896.2

#coef(glmnet(x[train,],Y[train],alpha=alpha_enet,lambda = lambda_min_ridge))

```


## 2j 
The elastic net model had the lowest test MSE. This makes sense because it has 2 tuning parameters to make more accurate predictors. 


## 2k 
I would say to focus on the predictors that lasso (with the min lambda) did not shrink to 0. If lasso shrunk predictor coefficients to 0, they were not statistically significant so focus on the predictors in lasso. So hits, walks, CHmRun, CRuns, CRBI, LeagueN, DivisionW, PutOuts, and Errors.  


# Problem 3: Bootstrap
## 3a 
population mean of medv:  
```{r}
muhat = mean(Boston$medv) 
muhat 

```

## 3b 
std error for the mean of medv:  
```{r}
n = dim(Boston)[1]
#summary(lm(crim~.,data=Boston))
#summary(lm(crim~.,data=Boston))$coef[2,2]
sd(Boston$medv)/sqrt(n)
```

## 3c 
std error for mean of medv using bootstrap:  
```{r}
n = dim(Boston)[1]
set.seed(1)
## bootstrap standard errors
B = 2000
mu_list = rep(0,2000)
for(b in 1:B){
  index = sample(1:n,n,replace=TRUE)
  bootsample = Boston[index,]
  mu_list[b] = mean(bootsample$medv)
}

se_boot = sqrt(sum((mu_list-mean(mu_list))^2)/(B-1)) 
se_boot 

```
This value are very similar but the bootstrap value is slightly smaller than the std error in part b.  


## 3d 
confidence interval for mean(medv): 
```{r}
##bootstrap confidence intervals
mu_star = mean(Boston$medv) 
se_mu_star = sd(Boston$medv)/sqrt(n) 

B = 500
m = 100
Fstar = rep(0,B)
mu_m = rep(0,m)

for(b in 1:B){
  index = sample(1:n,n,replace=TRUE)
  bootsample=Boston[index,]
  mu =  mean(bootsample$medv)
  for(i in 1:m){
    index2 = sample(index,n,replace=TRUE)
    bootsample2 = Boston[index2,]
    mu_m[i] = mean(bootsample2$medv)
  }
  se_mu = sqrt(sum((mu_m-mean(mu_m))^2)/(m-1))

  Fstar[b] = (mu - mu_star)/se_mu 
  #print(Fstar[b])
}

#95% confidence interval
#quantile(Fstar,c(0.025,0.975))
mu_star + quantile(Fstar,0.025)*se_mu_star
mu_star + quantile(Fstar,0.975)*se_mu_star
```

confidence interval using analytical formula: 
```{r}
# compare with analytical formulas
result = t.test(Boston$medv)
 
# Extract the confidence interval
confidence_interval = result$conf.int
 
# Print the confidence interval
confidence_interval

```
These intervals are very similar, with the bootstrap formula resulting in a slightly wider interval. This may change with different B and m values.  


## 3e 
median value for medv: 
```{r}
median(Boston$medv)

```

## 3f
se for median of medv: 
```{r}
n = dim(Boston)[1]
set.seed(1)
## bootstrap standard errors
B = 2000
median_list = rep(0,2000)
for(b in 1:B){
  index = sample(1:n,n,replace=TRUE)
  bootsample = Boston[index,]
  median_list[b] = median(bootsample$medv)
}

se_med_boot = sqrt(sum((median_list-mean(median_list))^2)/(B-1)) 
se_med_boot 

```
This is a fairly small standard error compared to the median.  


## 3g 
10th percentile of medv 
```{r}
quantile(Boston$medv, .1)
```

## 3h  
se for 10th percentile of medv 
```{r}
n = dim(Boston)[1]
set.seed(1)
## bootstrap standard errors
B = 2000
quant_list = rep(0,2000)
for(b in 1:B){
  index = sample(1:n,n,replace=TRUE)
  bootsample = Boston[index,]
  quant_list[b] = quantile(bootsample$medv, .1)
}

se_quant_boot = sqrt(sum((quant_list-mean(quant_list))^2)/(B-1)) 
se_quant_boot 

```
This is also a relatively small standard error. 



# Problem 4: Properties of Bootstrap
## 4a 
The probability that the first bootstrap observation is the jth observation from the original sample is 1/n, where n is the number of observation in the original sample. Since we are selecting our bootstrap sample with replacement, each selection is independent and we have an equally likely chance of picking any observation from our original sample.  

## 4b 
The probability that the first bootstrap observation is not the jth observation from the original sample is (n-1)/n or 1-(1/n), where n is the number of observation in the original sample. Since we are selecting our bootstrap sample with replacement, each selection is independent and we have an equally likely chance of picking any observation from our original sample.  

## 4c 
The probability that the jth observation from the original sample is not in the bootstrap sample would be ((n-1)/n)^n. Since the selections are independent of each other (because we have replacement) we can multiply the probability that j is not in each observation of the bootstrap.  

## 4d 
The probability that the jth observation from the original sample is in the bootstrap sample is 1-((n-1)/n)^n since it is complementary to the probability that it is not in the sample so,  
When n=5, the probability that the jth observation is in the bootstrap sample is 1-((5-1)/5)^5 = .67232  

## 4e 
When n = 100, what is the probability that the jth observation is in the bootstrap sample is 1-((100-1)/100)^100 = .63397    

## 4f 
When n = 10,000, what is the probability that the jth observation is in the bootstrap sample is 1-((10000-1)/10000)^10000 = .63214    

## 4g 
```{r}
prob = rep(NA, 100000)
for (i in 1:100000){
  n=i 
  p = 1-((n-1)/n)^n 
  prob[i]=p 
}
plot(prob, type='l')

```


## 4h 
The number of times 5 appeared in the bootstrap was 6399. If you divide that by the number of times the bootstrap sample was ran, the value is .6452 which is the probability of getting the value of 5 in the bootstrap. The more times this trial is run, the closer this value will get to the true probability.  
```{r, results='hide'}
n=10000
results <- rep(NA, 10000)
    for(i in 1:10000){
          results[i] <- sum(sample(1:100, rep=TRUE) == 5)>0
}
count = length(results[results==TRUE])
count/n

```










