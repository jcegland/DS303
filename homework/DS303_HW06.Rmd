---
title: "HW06"
author: "Jillian Egland"
date: "2023-09-30"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r, results = 'hide', message=FALSE}
library(ISLR2)
library(leaps)
library(ggplot2)
library(kableExtra)
library(car)
library(glmnet)
```



# Poblem 1 Follow-up to in-class activity 
## 1a 
They aren't the same because the first example (where the models for different genders only differed by the intercpet), assumes that bmi and age affect charges at the same rate for both males and females. The second example includes an interaction between gender and the rates of change for bmi and age. Both of those predictors will affect charges differently for males and females and will allows the slopes to be different.  

## 1b 
```{r, results = 'hide'}
insurance=read.csv("/Users/jillianeglandschool/Desktop/DS303/insurance.csv") 
insurance$gender = as.factor(insurance$gender)
insurance$smoker = as.factor(insurance$smoker)
insurance$region = as.factor(insurance$region)
str(insurance)
```
Original Part 2:  
```{r, echo = TRUE}
full_model = lm(charges~age+bmi+gender,data=insurance)
full_model_sum = summary(full_model)

```
males: -5642.36 + 243.19(age) + 327.54(bmi)  
females: -6986.82 + 243.19(age) + 327.54(bmi)  
  
Part 3:  
```{r, echo=TRUE}
insurance_m = insurance[insurance$gender=="male",]
insurance_f = insurance[insurance$gender=="female",]
model_m = lm(charges~age + bmi, data=insurance_m)
model_f = lm(charges~age + bmi, data=insurance_f)
sum_m = summary(model_m)
sum_f = summary(model_f)

```
males: -8012.79 + 238.63(age) + 409.87(bmi)  
females: -4515.22 + 246.92(age) + 241.32(bmi)  
  
New Part 2:  
```{r, echo=TRUE}
fit = lm(charges~age + bmi + gender + age*gender + bmi*gender, data=insurance)
coef(summary(fit))

``` 
males: -8012.79 + 238.63(age) + 409.87(bmi)  
females: -4515.22 + 246.92(age) + 241.32(bmi)  
These are now the same as Part 3.  

## 1c 
We can use test MSE to determine which model is better.  
```{r, results = 'hide'}
set.seed(13)
n = dim(insurance)[1]
train_index = sample(1:n,n*.9,replace=F)
train_insur = insurance[train_index,]
test_insur = insurance[-train_index,] 


M1 = lm(charges~age+bmi+gender,data=train_insur)
M2 = lm(charges~age + bmi + gender + age*gender + bmi*gender, data=train_insur) 

#m1tr = mean((train_insur$charges - M1$fitted.values)^2) 
m1ts = mean((test_insur$charges - predict(M1,newdata=test_insur))^2)
m2ts = mean((test_insur$charges - predict(M2,newdata=test_insur))^2)

m1ts
m2ts 
```
test MSE for model without interaction: 102090860  
test MSE for model with interactions:   101490654  
Since the test MSE is lower for the model with interactions, it is a better fit.  


# Problem 2: Predictions in the presence of multicollinearity 
## 2a 
I would say that multicollinearity would not affect the accuracy of the model. It only affects our ability to make inferences about our model. So we have limited power in our hypothesis tests, but multicollinearity does not affect the accuracy of the model itself.  

```{r}
## 2b 
error = rnorm (100, mean=0, sd=2)
B0=3 
B1=2
B2=4

set.seed(42)
x1 = runif(100)
x2 = 0.8*x1 + rnorm(100,0,0.1)


Y=B0 + B1*x1 + B2*x2 + error 
data = data.frame(cbind(x1, x2, Y))
```

## 2c 
test MSE for multicollinearity model:  
```{r}
n = dim(data)[1]
train_index = sample(1:n,n*.9,replace=F)
train = data[train_index,]
test = data[-train_index,] 

M1 = lm(Y ~ x1+x2, data = train)
#summary(fit)

#new_data = data_frame()
m1ts = mean((test$Y - predict(M1,newdata=test))^2)
m1ts

```

## 2d 
mean test MSE for multicollinearity model:  
```{r}
set.seed(NULL)
test_MSEs = rep(NA, 2500)
for (i in 1:2500){
  error = rnorm (100, mean=0, sd=2) 
  Y=B0 + B1*x1 + B2*x2 + error 
  data = data.frame(cbind(x1, x2, Y))
  new_fit = lm(Y ~ x1+x2, data = data) 
  test_MSEs[i] = mean((test$Y - predict(new_fit,newdata=test))^2) 
}
mean(test_MSEs)
hist(test_MSEs)

```
  
The histogram is skewed to the right and centered around 1.5 


## 2e 
correlation value between x1 and x2:  
```{r}

error = rnorm (100, mean=0, sd=2)
set.seed(24)
x1 = runif(100)
x2 = rnorm(100,0,1)

Y=B0 + B1*x1 + B2*x2 + error 

cor(x1,x2)

```

## 2f 
mean test MSE for uncorrelated model: 
```{r}
set.seed(NULL)
test_MSEs = rep(NA, 2500)
for (i in 1:2500){
  error = rnorm (100, mean=0, sd=2) 
  Y=B0 + B1*x1 + B2*x2 + error 
  data = data.frame(cbind(x1, x2, Y))
  new_fit = lm(Y ~ x1+x2, data = data) 
  test_MSEs[i] = mean((test$Y - predict(new_fit,newdata=test))^2) 
}
mean(test_MSEs)
hist(test_MSEs)

```
  
The histogram is still right skewed and is centered around 1.5 again.  

## 2g 
Since the test MSE's for both the correlated model and the uncorrelated model were centered around the same value, the accuracy of the model's prediction was not affected by the multicollinearity.  


# Problem 3 

```{r}
n = dim(College)[1]
set.seed(12)
#train_index = sample(1:n,n*.9,replace=F)
train_index = sample(1:n,n/2,replace=F)
train_college = College[train_index,]
test_college = College[-train_index,] 

```

## 3b 
It is important to scale the predictors to the same standard deviation because you want the penalty to affect the predictors the same. Adding 50 to a predictor that is hundreds of thousands of dollars would be very different to adding 50 to a predictor of age.  
```{r}
train_x = model.matrix(Apps~.,data=train_college)[,-1] 
train_y = train_college$Apps
grid = 10^seq(10,-2,length=100)
ridge.train = glmnet(train_x,train_y,alpha=0, lambda=grid) 


```

## 3c 
best lambda value for ridge:  
```{r}
set.seed(12)
cv.out = cv.glmnet(train_x,train_y, alpha = 0, lambda = grid, nfolds=5) 
bestlambda = cv.out$lambda.min
bestlambda


#plot(cv.out)
```

## 3d 
l2 norm for ridge:  
```{r}
final_model = glmnet(train_x,train_y,alpha=0, lambda=bestlambda) 
coefficients_final = coef(final_model)[-1]

sqrt(sum(coefficients_final^2))

```

## 3e 
test MSE for ridge model with optimal lamda:  
```{r}
test_x = model.matrix(Apps~.,data=test_college)[,-1] 
test_y = test_college$Apps
ridge.pred = predict(ridge.train,s=bestlambda,newx=test_x)
mean((ridge.pred-test_y)^2)

```


## 3f 
best lambda value for lasso:  
```{r}
lasso.train = glmnet(train_x,train_y,alpha=1, lambda=grid) 
set.seed(12)
cv.out = cv.glmnet(train_x,train_y, alpha = 1, lambda = grid, nfolds=5) 
#default performs 10-fold CV, but you can change this using the argument `nfolds` 
bestlambda = cv.out$lambda.min
bestlambda

#plot(cv.out)

```

## 3g 
l1 norm for lasso:  
```{r}
final_model = glmnet(train_x,train_y,alpha=1, lambda=bestlambda) 
coefficients_final = coef(final_model)[-1]

sum(abs(coefficients_final)) 

```

## 3h 
test MSE for lasso model with optimal lamda:  
```{r}
test_x = model.matrix(Apps~.,data=test_college)[,-1] 
test_y = test_college$Apps
lasso.pred = predict(lasso.train,s=bestlambda,newx=test_x)
mean((lasso.pred-test_y)^2)

```

## 3i 
test MSE for full regular linear regression model:  
```{r}
fit = lm(Apps~., data=train_college)
m1ts = mean((test_college$Apps - predict(fit,newdata=test_college))^2)
m1ts

```
The test MSE is slightly higher for the lasso regression (1,417,067) than for ridge regression (1,416,737), which is slightly higher than the test MSE for the full regular linear regression model (1,416,217). So,the test MSEs for each model are comparable.  







