---
title: "HW03"
author: "Jillian"
date: "2023-09-01"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


# Problem 1: Statistical Inference 

## 1a  
*Fit a multiple linear regression model to predict carseat unit sales (in thousands) using all other variables except ShelveLoc as your predictors. Use the entire dataset (do not split it into a training and test set). Summarize your least-square estimates and their standard errors in a table. Choose one regression coefficient from the model and test whether it is zero or not at α = 0.05. Write out the null/alternative hypothesis, test statistic, null distribution, p-value, and conclusion.*  
```{r, results = 'hide'} 
library(ISLR2)
library(kableExtra)
head(Carseats)
```

```{r}
fit = lm(Sales~.-ShelveLoc, data=Carseats) 
results = summary(fit)$coefficients[,1:2]

kable(results, format = "html") %>% kable_styling() 

```

```{r, results='hide'}
summary(fit)$coefficients[7, ]
# df is n-p+1 
``` 
hypothesis test for Age:  
H0: B6 = 0  
Ha: B6 ≠ 0  
test statistic: -7.437  
null distribution: t distribution with 390 degrees of freedom
Conclusion: we find evidence of a relationship between Sales and Age at confidence level .05  



## 1b 
*Report an estimate for σ2. What does this value in plain language?*  

sigma squared is 3.733. This is the MSE, which is the sum of the squared difference between the training model and the actual data. This quantifies the error in your model.  

```{r, results = 'hide'} 
summary(fit)$sigma^2

```


## 1c
*Carefully interpret the estimated regression coefficient associated with Advertising. Double
check your lecture notes for precise language.*  

0.1369399 is the average change in sales associated with 1 unit change in advertising, holding all other predictors constant.  


## 1d  
*Obtain the RSS for the full model and the RSS for reduced model. Report them both here.*  
**wrong, RSS full=1456.031, RSS reduced=3182.275 

RSS full: 1.932 
RSS reduced: 2.824  


```{r, results='hide'} 
fit_reduced = lm(Sales~1, Carseats)

summary(fit_reduced)
```


## 1e  
*Assume that our random errors (εi) are normally distributed. Carry out the F-test at α = 0.05. Write out the null/alternative hypothesis, test statistic, null distribution, p-value, and conclusion.*  

H0: B1 = B2 = ... = B9 = 0  
Ha: at least one Bj ≠ 0  
test statistic: 51.38  
null distribution: F distribution 9,390 
p-value: < 2.2e-16  
conclusion: we find evidence of a relationship between sales and at least one predictor at confidence level .05  


## 1f 
*Use the model to estimate f(X) when the price charged by competitor is average (you’ll need to find what the average competitor price is), median community income level, advertising is 15, population is 500, price for car seats at each site is 50, average age of local population is 30, education level is 10, and the store is in an urban location within the US. What is your estimate for f(X) given these predictor value? Quantify the uncertainty surrounding our estimate for f(X) by reporting the appropriate interval.*  

f(X) estimate: 15.80707  
uncertainty: between 14.91953 and 16.69461 with 95% confidence  


```{r, results = 'hide'} 

Xh = data.frame(CompPrice = mean(Carseats$CompPrice), Income = median(Carseats$Income), Advertising = 15, Population = 500, Price = 50, Age = 30, Education = 10, Urban = 'Yes', US = 'Yes', ShelveLoc='Bad')
predict(fit,newdata=Xh,interval='confidence',level=0.95)

```


## 1g 
*Same setting as part (f). What is your prediction for Y given these predictors? Quantify the uncertainty surrounding our prediction for Y (given these predictors) by reporting the appropriate interval.*  

Y prediction: 15.80707  
uncertainty: between 11.90593 and 19.70821 with 95% confidence  

```{r, results = 'hide'} 
predict(fit,newdata=Xh,interval='prediction',level=0.95)


```

## 1h  
*Obtain the prediction for Y using all the same settings as (f), but set the price for car seats at each site to be 450. What is your prediction for Y ? Does this value make sense? Discuss how this reveals a limitation of our model.*  

Y prediction: -21.16873  
This does not make sense as sales would not be negative. The model will have some limitations as when we try to predict for values outside the ones in the training set, it will be extrapolation.  

```{r, results = 'hide'} 
Xh = data.frame(CompPrice = mean(Carseats$CompPrice), Income = median(Carseats$Income), Advertising = 15, Population = 500, Price = 450, Age = 30, Education = 10, Urban = 'Yes', US = 'Yes', ShelveLoc='Bad')
predict(fit,newdata=Xh,interval='prediction',level=0.95)

```


# Problem 2: The Challenge of Multiple Testing 
*Think back to our in-class activity related to multiple testing (see R script multiple testing.R if you need a refresher). We illustrated in that code that if that set α = 0.05, we would expect roughly 10 predictors to be significant just by chance, and we know some of those significant predictors are false positives. This illustrates the multiple testing problem: when testing a large number of null hypothesis, we are bound to get some very small p-values just by chance. If we make a decision about whether to reject each H0, without accounting for the fact that we have performed many tests, we may end up making a large number of type 1 errors (also referred to as false positives or false discoveries).*  

## 2a  
*In general if we wish to test m null hypothesis and we simply reject all null hypothesis for which the corresponding p-value falls below α, how many type 1 errors should we expect to make?*  

we can expect to make m*α type 1 errors.  

## 2b  
*Suppose we are carrying out m hypothesis test and we reject H0 if its corresponding p-value falls below α (i.e. controlling Type 1 error for each null hypothesis at level α). Let V represent the number of type 1 errors. For m hypothesis tests, calculate P (V ≥ 1). Your value should be expressed in terms of m and α.*  
*This probability P(V ≥ 1) is called the family-wise error rate (FWER). Ideally, we would like the FWER to be controlled at α such that FWER ≤ α.*  

P(V ≥ 1) = 1 - P(no significance) = 1 - (1-α)^m  

## 2c 
*Describe a data application/setting where the multiple testing problem could be especially problematic if not addressed properly.*  

Multiple testing could be a problem in healthcare. That is a very serious setting in which being correct is important. If we test a model that says that these predictors are statistically significant when they are not actually significant, our results will not be accurate, which is dangerous when it comes to a person's health.  

## 2d  
*Your colleague suggests a very simple approach to address this multiple testing problem. They suggest that instead of using α as our cutoff for rejecting H0, we use α/m (where m is the number of hypothesis testings we are carrying out). Does your colleague’s approach make statistical sense? Show that your colleague’s approach controls the FWER at α (in other words FWER ≤ α).*  

The approach makes statistical sense. The more estimates we have, the more likely we are to make a type one error so dividing by the number of estimates accounts for that number. So with 100 tests at α=.05, the chance of at least one false positive is now 1 - (1-.0005)^100 = .04878 instead of .994  

## 2e  
*Repeat our in-class activity using your colleague’s approach in (d). How many predictors are significant using the cutoff of α/m?*  

6 significant values as opposed to 16 beforehand. 

```{r, results='hide'} 
set.seed(10)
x = matrix(NA,1000,200)

for(i in 1:200){
  x[,i] = rnorm(1000)
}

beta0 = 1 
beta1 = 2 
beta2 = 3 
beta3 = 4 
beta4 = 5 
beta5 = 6

y = beta0 + beta1*x[,1] + beta2*x[,2] + beta3*x[,3] + beta4*x[,4] + beta5*x[,5] + rnorm(1000, 0, 1)

#x[,196] = y*2
#x[,197] = y*3
#x[,198] = y + 1500
#x[,199] = y/5
#x[,200] = y^2 

data = as.data.frame(cbind(y,x)) 

fit = lm(y~.,data=data)
#summary(fit)

p_values = summary(fit)$coefficients[,4]  
length(which(p_values<0.05))
length(which(p_values<0.05/5))



```

## 2f
*List one potential drawback of your colleague’s approach in (d).*  
The cutoff is strinc and makes it so that we have less power to reject the null hypothesis. 


# Problem 3: Diagnostics for MLR 
## 3a 
*List the four assumptions we make when fitting a multiple linear regression model.*  

1. relationship between x and Y is approximately linear  
2. E(e) = 0, expected random noise is 0  
3. Var(e) = σ^2, constant variance assumption  
4. Errors are uncorrelated, don't affect each other  

## 3b 
*True or False? In order to fit a multiple linear regression model (i.e. obtain least square estimates), we must have distributional assumptions on the random error term εi. Briefly justify your answer.*  

False, we do not need to make any assumptions about the error distribution for linear models. We do have to assume that the errors have a mean of 0 and have constant variance. We only have to assume normal error distribution when making inferences such a t tests or confidence intervals.  

## 3c 
*True or False? The lm() output automatically gives you p-value for individual hypothesis tests: H0 : βj = 0 versus H1 : βj ̸= 0, j = 1,...,p. These hypothesis tests assume that the random error term are normally distributed. Briefly justify your answer.*  

True, these tests assume that the random errors are normally distributed. We have to assume this in able to make inferences about the t distribution.  

## 3d 
*Suppose we want to run some visual diagnostics to check whether or not the linearity as- sumption holds. Besides the fact that it is is time consuming, explain why it is not sufficient to check every pairwise scatterplot between Y and Xj , j = 1, . . . , p. Hint: think back to HW 1, Problem 3(j).*  

Comparing every variable to the response variable may show some sort of correlation, but will not show enough variance between the predictors. The single predictors might show low p values for each one, but you won't be able to see enough variation to say that one predictor is better than another. It also will not be able to pick up the interaction that the variable may have with each other when in the same model. 

## 3e 
*To check whether or not the linearity assumption holds, we can run some diagnostics. Let’s use the Auto dataset as an example. Run the following code:*  
```{r} 

m1 = lm(mpg~horsepower,data=Auto)
#summary(m1)
par(mfrow=c(2,2))
plot(m1)


```
*Take a look at the plot in the top left corner (titled Residuals vs Fitted). This is what we call a residual plot because it is literally a plot of residuals versus fitted values (yˆi). The red line is a smooth fit to the residuals, which is displayed in order to make it easier to identify any trends. You should observe that the plot exhibits a clear U-shape, which provides a strong indication of non-linearity in the data.*  
*Ideally what we want to see is no discernible pattern in the residual plot and a relatively flat red line. A residual plot exhibiting no pattern and a flat red line indicates that the linearity assumption for our model holds. Propose a more flexible model than m1 that might be able to accommodate some non-linearity. Present your proposed model (call it m2) and its corresponding residual plot here. Is it an improvement over m1?*  


I propose using a model with a higher complexity (I used 2) to try to account for the nonlinearity. The red line in the residual plot is more flat than the original plot but not completely flat, so it is a slight improvement.  

```{r} 
m2 = lm(mpg~poly(horsepower,2,raw=TRUE),data=Auto)
par(mfrow=c(2,2))
plot(m2)

```



