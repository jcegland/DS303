---
title: "HW08"
author: "Jillian Egland"
date: "2023-10-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
```{r, message=FALSE}
library(ggplot2)
library(ISLR2)
library(MASS)
library(e1071)
```

# Problem 1: Concept Review
## 1a 
*Explain in plain language how we obtain estimates for β when fitting a logistic regression model.*  

To obtain estimates for Beta when fitting a logistic regression model, we use the maximum likelihood estimation to minimize Beta hats. The estimation tells us the probability of seeing the data we do given the X parameters. So we pick beta values that maximize those probabilities. We also use the sigmoid function to restric the function to be between 0 and 1.  

## 1b 
*When using logistic regression, what threshold will give us the smallest overall misclassifica- tion rate? Explain briefly why.*  

using .5 will give us the smallest overall misclassification rate because it mimics Bayes rule which gives us the lowest possible error.  

## 1c 
*Suppose we are trying to build a classifier where Y can take on two classes: ‘sick’ or ‘healthy’. In this context, we consider a positive result to be testing sick (you have the virus) and a negative result to test as healthy (you don’t have the virus). After fitting the model with LDA in R, we compare how our classifier performs with the actual outcomes of the individuals, as shown below:*  
*What is the misclassification rate for the LDA classifier above? In the context of this problem, which is more troubling: a false positive or a false negative? Depending on your answer, how could you go about decreasing the false positive or false negative rate? Comment on how this will likely affect overall the misclassification rate (consider which threshold will have the lowest overall misclassification rate).*  

Misclassification rate: 57/218 = .261  
In this medical context, it is more troubling to classify someone as healthy when they are actually sick than it is to classify someone as sick when they are healthy. So a false negative is more troubling.  
To decrease the false negative rate, we can decrease the threshold level to >.25 or something less than .5, this way we are more likely to classify someone as sick even if they aren't which errs of the safer side.  
This will increase our misclassification rate slightly since we are deviating from .5  

## 1d 
*Suppose we collect data for a group of students in a statistics class with variables X1 = hours studied, X2 = undergrad GPA, and Y = receive an A. We fit a logistic regression model and produce estimated regression coefficients, βˆ0 = −6, βˆ1 = 0.05, and βˆ2 = 1.*  
i)  
*Estimate the probability that a student who studies for 40 hours and has an undergrad GPA of 3.5 gets an A in the class.*  
P(Y=1)|X1=40, X2=3.5) = exp(beta0+beta1\*X1 + beta2\*X2)/(1+exp(beta0+beta1\*X1 + beta2\*X2)) = 
```{r}
beta0=-6 
beta1=.05 
beta2=1 
X1=40 
X2=3.5 

exp(beta0+beta1*X1 + beta2*X2)/(1+exp(beta0+beta1*X1 + beta2*X2)) 


```
ii) 
*How many hours would the student in part (i) need to study to have a 50% chance of getting an A in the class?*  
student in part (i) would need to study 50 hours to get a 50% chance of getting an A.  
```{r, results = 'hide'}
beta0=-6 
beta1=.05 
beta2=1 
X1=50 
X2=3.5 

exp(beta0+beta1*X1 + beta2*X2)/(1+exp(beta0+beta1*X1 + beta2*X2)) 
```

## 1e 
*If the true decision boundary between two groups is linear and the constant variance assump- tion holds, do you expect LDA or QDA to perform better on the testing set? Explain using concepts from bias/variance tradeoff.*  

I would expect LDA to perform better on the testing set. QDA is a more flexible model and would be overfitting to the training set because only a linear model is needed. QDA would have much more variance. 

## 1f 
*Same setup as (e), do you expect LDA or QDA to perform better on the training set? Justify your answer.*  

I would expect QDA to perform better on the training set. Since only a linear model is needed, QDA would overfit to the training data and would have a lot of variance. 

## 1g 
*Create a data set that consists of two predictors (X1, X2) and a binary response variable Y . Let n = 16 and Y = 0 for 8 observations and Y = 1 for the remaining 8 observations. Create this data set in such a way that logistic regression cannot converge when applied to this data set. Explain why logistic regression cannot converge on this data set. Report your estimates for β0, β1, and β2 obtained from fitting a logistic regression model on this data set. You may copy/paste your output.*  

coefficient estimates for betas:  
```{r}
set.seed(1) 
x1 = sample(16)
x2=sample(16)
y=rep(1, 16)
y[x1<8]=0

data = data.frame(x1,x2,y)
#str(data)


#ggplot(data, aes(x=x2, y=y, color=y)) + geom_point(size=2)

glm.fit = glm(y~x1+x2, data=data,family='binomial')

coef(summary(glm.fit))
#summary(glm.fit)

#glm.pred = rep('No',length(test))
#glm.pred[glm.prob >0.5] ='Yes'
#table(glm.pred,Default[test,]$default)
#1-mean(glm.pred == Default[test,]$default)

```
logistic regression cannot be converge on this dataset because the classes are well separated. 


## 1h 
*Apply LDA/QDA to the dataset you created in part (h). Are you able to get meaningful results? Report the misclassification rate for LDA and QDA.*  

the misclassification rate for lda and qda were both 0. We were able to get meaningful results because lda and qda account for the limitations in logistic when the data is evenly split. The misclassification rate would be 0 because the data is evenly split.    
```{r, results='hide'}
data$y = as.factor(data$y)

lda.fit = lda(y~x1+x2,data=data)
lda.pred = predict(lda.fit,data)
mean(lda.pred$class!=y)

## QDA 
qda.fit = qda(y~x1+x2,data=data)
qda.pred = predict(qda.fit,data)

mean(qda.pred$class!=data$y)


```


# Problem 2: Email Spam 
*The last column of the spam data set, called V58, denotes whether the e-mail was considered spam (1) or not (0).*  

## 2a 
*What proportion of emails are classified as spam and what proportion of emails are non-spam?*  

Spam: .394  
Not Spam: .606  
```{r, results = 'hide'}
spam = read.csv("/Users/jillianeglandschool/Desktop/DS303/spambase.data",header=FALSE)


#considered spam 
nrow(spam[spam$V58=='1',])/nrow(spam)

#not considered spam 
nrow(spam[spam$V58=='0',])/nrow(spam)


```

## 2b 
*Carefully split the data into training and testing sets. Check to see that the proportions of spam vs. non-spam in your training and testing sets are similar to what you observed in part (a). Report those proportions here.*  

the proportions in the training and test set are similar to the original dataset  
train spam: 0.403  
train not spam: 0.597  
test spam: 0.385  
test not spam: 0.614  
```{r}
set.seed(1)
n = dim(spam)[1]
train_index = sample(1:n,n/2,replace=F)
train_spam = spam[train_index,]
test_spam = spam[-train_index,] 

#considered spam 
nrow(train_spam[train_spam$V58=='1',])/nrow(train_spam)
#not considered spam 
nrow(train_spam[train_spam$V58=='0',])/nrow(train_spam)

#considered spam 
nrow(test_spam[test_spam$V58=='1',])/nrow(test_spam)
#not considered spam 
nrow(test_spam[test_spam$V58=='0',])/nrow(test_spam)


```

## 2c 
*Fit a logistic regression model here and apply it to the test set. Use the predict() function to predict the probability that an email in our data set will be spam or not. Print the first ten predicted probabilities here.*  

P(Y=1) for the first 10 observations: 
```{r, warning=FALSE}
glm.fit = glm(V58~., data=train_spam, family='binomial')

glm.prob = predict(glm.fit,test_spam,type='response') 
head(glm.prob, 10)

```

## 2d 
*We can convert these probabilities into labels. If the predicted probability is greater than 0.5, then we predict the email is spam (Yˆi = 1), otherwise it is not spam (Yˆi = 0). Create a confusion matrix based on your results. What’s the overall misclassification rate? Break this down and report the false negative rate and false positive rate.*  

overall misclassification: 0.0734  
false negative rate: .1027  
false positive rate: .0551  
```{r}
glm.pred = rep('0',nrow(test_spam))
glm.pred[glm.prob >0.5] ='1'
table(glm.pred,test_spam$V58)

1-mean(glm.pred == test_spam$V58)

#rows are predicted, # columns are true 

# false positive rate: FP/(FP+TN) = 78/(78+1337) = .0551 
# false negative rate: FN/(FN+TP) = 91/(91+795) = .1027 
```

## 2e 
*What type of mistake do we think is more critical here: reporting a meaningful email as spam or a spam email as meaningful? How can we adjust our classifier to accommodate this?*  

It would be worse to label an email as spam when it wasn't than to label an email as not spam when it was spam. We want to increase our threshold past .5 so that we make sure that an email really is spam before labeling it as so. In doing so our overall misclassification rate is now .0952 but our false positive rate is now .0318  
```{r}
glm.pred = rep('0',nrow(test_spam))
glm.pred[glm.prob >0.75] ='1'
#table(glm.pred,test_spam$V58)

#1-mean(glm.pred == test_spam$V58)

```

# Problem 3: Weekly data set
## 3a 
*Use the full data set to perform a logistic regression with Direction as the response and the five lag variables plus Volume as predictors. Use the summary function to report your results.*  

```{r}
glm.fit = glm(Direction~.-Year-Today, data=Weekly, family='binomial')
summary(glm.fit)
```

## 3b 
*Set a threshold that minimizes the overall misclassification rate. Compute the confusion matrix and overall correct classification rate. Explain what the confusion matrix is telling you about the types of mistakes made by logistic regression.*  

Overall correct classification rate: .5611  
Confusion matrix:  
```{r}
glm.prob = predict(glm.fit,Weekly,type='response') 
glm.pred = rep('Down',nrow(Weekly))
glm.pred[glm.prob >0.5] ='Up'
table(glm.pred,Weekly$Direction)

## correct classification rate 
#mean(glm.pred == Weekly$Direction)


```
false positives: 430/(430+54) = .8884  
false negatives: 48/(48+557)= .0793 
This logistic regression makes more false positives than false negatives. So it predicts more to be up when they are actually down than it does predicting them as down when they are actually up.  


## 3c 
*Now fit the logistic regression model using a training data period from 1990 to 2008, with Lag2 as the only predictor. Set a threshold that minimizes the overall misclassification rate. Compute the confusion matrix and overall correct classification rate on the test set (that is, data from 2009 and 2010).*  

overall correct classification rate: 0.625  
```{r}
train_weekly = Weekly[Weekly$Year>=1990 & Weekly$Year<=2008, ]
test_weekly = Weekly[Weekly$Year>2008, ] 

glm.fit = glm(Direction~Lag2, data=train_weekly, family='binomial')
#summary(glm.fit)
glm.prob = predict(glm.fit,test_weekly,type='response') 
glm.pred = rep('Down',nrow(test_weekly))
glm.pred[glm.prob >0.5] ='Up'
table(glm.pred,test_weekly$Direction)

mean(glm.pred == test_weekly$Direction)

```


## 3d 
*Repeat (c) using LDA.*  

LDA:  
```{r}
lda.fit = lda(Direction~Lag2,data=train_weekly)
lda.pred = predict(lda.fit,test_weekly)
table(lda.pred$class,test_weekly$Direction)
mean(lda.pred$class==test_weekly$Direction)

```


## 3e
*Repeat (c) using QDA.*  

QDA:  
```{r}
qda.fit = qda(Direction~Lag2,data=train_weekly)
qda.pred = predict(qda.fit,test_weekly)

table(qda.pred$class,test_weekly$Direction)

mean(qda.pred$class==test_weekly$Direction)

```


## 3f 
*Repeat (c) using Naive Bayes.*  

Naive Bayes:  
```{r}
nb.fit = naiveBayes(Direction~Lag2, data=train_weekly)
#nb.fit

nb.class = predict(nb.fit, test_weekly)
table(nb.class, test_weekly$Direction)
mean(nb.class == test_weekly$Direction)

```






